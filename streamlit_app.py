import base64
from transformers import VitsTokenizer, VitsModel
import streamlit as st
from llava_model import load_model, inference
from text_to_speech_model import text_to_speech, load_tts_model
from typing import Any
from PIL import Image


def st_describe(model: Any, tts: VitsModel, tts_tokenizer: VitsTokenizer, prompt: str, img: Image):
    """
    Describes the image with a prompt in the browser.

    Args:
        model (Any): The LLaVa model.
        tts (VitsModel): The text-to-speech model.
        tts_tokenizer (VitsTokenizer): The text-to-speech tokenizer.
        prompt (str): The prompt expected from the user.
        img (PIL.Image): The image to be described
    """
    with st.spinner('Describing the image...'):
        resp = inference(model, prompt, img)

    st_generate_audio(tts, tts_tokenizer, resp)


def st_generate_audio(tts: VitsModel, tts_tokenizer: VitsTokenizer, text: str) -> None:
    """
    Generates the audio transcription of the text description and plays the audio.

    Args:
        tts (VitsModel): The text to speech model
        tts_tokenizer (VitsTokenizer): The text-to-speech tokenizer.
        text (str): The image description
    """
    with st.spinner('Generating the audio file...'):
        wav_data = text_to_speech(text, tts, tts_tokenizer)

    st_autoplay(wav_data)


def st_autoplay(wav: bytes) -> None:
    """
    Plays the wav file generated by the text to speech model.

    Args:
        wav (bytes): The wav file to be played.
    """
    b64 = base64.b64encode(wav).decode()

    md = f"""
             <audio controls autoplay="true">
             <source src="data:audio/mp3;base64,{b64}" type="audio/wav">
             </audio>
             """

    st.markdown(md, unsafe_allow_html=True)


def main():
    """ Main app """
    st.title('Welcome to VAVI - Visual Assitant for Visually Impaired!')
    st.markdown(
        """
        <body>
        <h4> Either upload an image or use the camera to capture an image to describe.</h4>
        </body>
        """,
        unsafe_allow_html=True
    )

    with st.spinner('Loading the models, please wait'):
        llama_model = load_model()
        tts_model, tts_tokenizer = load_tts_model()

    img_file_buffer_c = st.camera_input("Take a picture")

    # https://docs.streamlit.io/library/api-reference/widgets/st.file_uploader
    img_file_buffer_i = st.file_uploader("Upload a picture")

    img_file_buffer = img_file_buffer_c if img_file_buffer_c is not None else img_file_buffer_i

    if img_file_buffer:
        cam_image = Image.open(img_file_buffer)

        # bytes_data = img_file_buffer.getvalue()
        # st.write(bytes_data)

        if st.button('Describe the image'):
            st_describe(llama_model, tts_model, tts_tokenizer, "Please describe the image.", cam_image)
        if st.button('Read the label'):
            st_describe(llama_model, tts_model, tts_tokenizer, "Read the text on the image. If there is no text, "
                                                               "write that the text cannot be found.", cam_image)


if __name__ == '__main__':
    main()
